{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filenames\n",
    "def get_filenames_year(which_year, filename_pre):\n",
    "    if (which_year == '2018'):\n",
    "        filenames = [filename_pre + which_year + '_q1'+ file_format,\n",
    "                     filename_pre + which_year + '_q2'+ file_format,\n",
    "                     filename_pre + which_year + '_q3'+ file_format]\n",
    "    elif (which_year == '2016') | (which_year == '2017'): \n",
    "         filenames = [filename_pre + which_year + '_q1'+ file_format,\n",
    "                      filename_pre + which_year + '_q2'+ file_format,\n",
    "                      filename_pre + which_year + '_q3'+ file_format,\n",
    "                      filename_pre + which_year + '_q4'+ file_format]\n",
    "    elif (which_year == '2015'): \n",
    "         filenames = [filename_pre + which_year + '_q2'+ file_format,\n",
    "                      filename_pre + which_year + '_q3'+ file_format,\n",
    "                      filename_pre + which_year + '_q4'+ file_format]\n",
    "    return filenames\n",
    "\n",
    "def create_and_clean_df(which_year, filenames, column_names, min_duration, max_duration_sd):     \n",
    "    df = pd.DataFrame(columns = column_names)\n",
    "    for x in range(len(filenames)): \n",
    "        current_file = os.path.join(data_dir + '/' + filenames[x])\n",
    "        temp = pd.read_csv(current_file)\n",
    "        df = df.append(temp)\n",
    "    \n",
    "    # 1) Drop all trips for which there is no duration information\n",
    "    df = df[pd.notnull(df['duration'])]\n",
    "\n",
    "    # 2) Identify potentially anomalous trips (based on length) \n",
    "    max_duration = df[\"duration\"].mean() + df[\"duration\"].std()*max_duration_sd\n",
    "    num_dropped_bottom = 100*(len(df[df['duration']<=min_duration]))/(len(df['duration']))\n",
    "    num_dropped_top = 100*(len(df[df['duration']>=max_duration]))/(len(df['duration']))\n",
    "\n",
    "    # 3) Drop outliers within a q(and print proportions)\n",
    "    print('For year %s: ' %which_year)\n",
    "    df = df[(df.duration < max_duration) & (df.duration > min_duration)]\n",
    "    print(\"Percent of trips that are too short %0.1f.\" %num_dropped_bottom)\n",
    "    print(\"Percent of trips that are too long %0.1f.\" %num_dropped_top)\n",
    "    \n",
    "    # 1) Drop all trips for which there is no duration information\n",
    "    df = df[pd.notnull(df['start_station'])]\n",
    "\n",
    "    # Drop all trips for which we don't have the station number. \n",
    "    # Identify unknown stations and drop them\n",
    "    df['start_station_id'] = df['start_station'].astype(int)\n",
    "    df['end_station_id'] = df['end_station'].astype(int)\n",
    "    #df = df[(df.start_station_id > station_numbers[0]) & (df.start_station_id < station_numbers[1])]\n",
    "    #df = df[(df.end_station_id > station_numbers[0]) & (df.end_station_id < station_numbers[1])]\n",
    "    \n",
    "    # Reformat the starttime, so that its rounder per hour\n",
    "    df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "    df['start_time'] = df['start_time'].dt.round(\"H\")\n",
    "    # Extract date and hour from the start date to end date range\n",
    "    df['start_time_date'] = pd.to_datetime(df['start_time']).dt.to_period('D')\n",
    "    df['start_time_hour'] = pd.DatetimeIndex(df['start_time']).hour\n",
    "    return df\n",
    "\n",
    "def manipulate_df(this_df, timeseries, which_station, which_year): \n",
    "    temp_df = this_df[this_df['start_station_id'] == which_station].reset_index()\n",
    "    temp_df = temp_df.drop(columns = 'index')\n",
    "    df = pd.DataFrame({'count':temp_df.groupby(['start_time']).size()}).reset_index()\n",
    "    df = df.set_index('start_time')\n",
    "    df = df.reindex(timeseries, fill_value = 0)\n",
    "    # add date and time separately as columns\n",
    "    # make a column out of an index\n",
    "    df['timestamp'] = df.index\n",
    "    df['start_time_date'] = pd.to_datetime(df['timestamp']).dt.to_period('D')\n",
    "    df['start_time_hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns = 'index')    \n",
    "    # save into dataframe\n",
    "    filename = 'Station'+ str(int(which_station)) + '-' + which_year + file_format\n",
    "    df.to_csv(filename)\n",
    "\n",
    "def make_timeseries_old(timeseries, column_names): \n",
    "# for end time\n",
    "# format a timeseries dataframe so we can join it with the trips\n",
    "    first_col = column_names[0]\n",
    "    second_col = column_names[1]\n",
    "    t_df = pd.DataFrame(index=timeseries_format, columns=column_names).reset_index()\n",
    "    column_names.insert(0, 'timestamp')\n",
    "    t_df.columns = column_names\n",
    "    t_df[first_col] = pd.to_datetime(t_df['timestamp']).dt.to_period('D')\n",
    "    t_df[second_col] = pd.DatetimeIndex(t_df['timestamp']).hour\n",
    "    # drop timestamp, we don't need it anymore\n",
    "    t_df = t_df.drop(columns = 'timestamp')\n",
    "    return t_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directories and other file parameters\n",
    "main_dir = os.getcwd() \n",
    "data_dir = main_dir + '/data'\n",
    "filename_pre = 'indego_trips_'\n",
    "file_format = '.csv'\n",
    "year_range = ['2015','2016','2017','2018'] \n",
    "column_names = ['trip_id', 'duration', 'start_time', 'end_time','start_station','start_lat','start_lon', \n",
    "                    'end_station','end_lat','end_lon', 'bike_id', 'plan_duration', 'trip_route_category', 'passholder_type']\n",
    "\n",
    "which_year = '2017'\n",
    "filenames = get_filenames_year(which_year, filename_pre)\n",
    "station_numbers = [3000, 4000]\n",
    "min_duration = 1 # 1 minute\n",
    "max_duration_sd = 2 # 2 standard deviations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For year 2017: \n",
      "Percent of trips that are too short 1.3.\n",
      "Percent of trips that are too long 1.2.\n"
     ]
    }
   ],
   "source": [
    "df = create_and_clean_df(which_year, filenames, column_names, min_duration, max_duration_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a timeseries array\n",
    "first_day = df['start_time_date'].min().to_timestamp()\n",
    "last_day = df['start_time_date'].max().to_timestamp()  + timedelta(hours=23)\n",
    "timeseries_format =  pd.date_range(first_day, last_day, freq='H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most valuable stations in rank-order are:  [3023, 3010, 3021, 3045, 3032, 3054, 3020, 3012, 3022, 3057]\n"
     ]
    }
   ],
   "source": [
    "# Identify most valuable stations\n",
    "station_df = df.groupby(['start_station'])['trip_id'].count() #identify unique trips and count them\n",
    "how_many = 10\n",
    "most_valuable_stations = station_df.sort_values(ascending = False).head(how_many).index.tolist()\n",
    "print(how_many,\"most valuable stations in rank-order are: \", most_valuable_stations)\n",
    "# Save the set of stations if it's for 2018 year\n",
    "if which_year == 2018: \n",
    "    import pickle\n",
    "    f = open('most_valuable_stations.pckl', 'wb')\n",
    "    pickle.dump(most_valuable_stations, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in most_valuable_stations: \n",
    "    manipulate_df(df, timeseries_format, station, which_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_df = df\n",
    "which_station = 3010\n",
    "timeseries = timeseries_format\n",
    "temp_df = this_df[this_df['start_station_id'] == which_station].reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = temp_df.drop(columns = 'index')\n",
    "df = pd.DataFrame({'count':temp_df.groupby(['start_time']).size()}).reset_index()\n",
    "df = df.set_index('start_time')\n",
    "df = df.reindex(timeseries, fill_value = 0)\n",
    "    # add date and time separately as columns\n",
    "    # make a column out of an index\n",
    "df['timestamp'] = df.index\n",
    "df['start_time_date'] = pd.to_datetime(df['timestamp']).dt.to_period('D')\n",
    "df['start_time_hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
    "df = df.reset_index()\n",
    "df = df.drop(columns = 'index')    \n",
    "# save into dataframe\n",
    "filename = 'Station'+ str(int(which_station)) + '-' + which_year + file_format\n",
    "df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
